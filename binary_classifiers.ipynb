{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_from_json(file_path):\n",
    "    with open(file_path) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        return data\n",
    "    \n",
    "def get_data(event_dir, datum):\n",
    "    \n",
    "    news_list = []\n",
    "    dataset_path = event_dir\n",
    "    data = data_from_json(event_dir)\n",
    "    \n",
    "    for key in data:\n",
    "        news_list.append(data[key][datum])\n",
    "            \n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = get_data('train.json','text')\n",
    "\n",
    "dev_data = get_data('dev.json','text')\n",
    "dev_label = get_data('dev.json','label')\n",
    "\n",
    "test_data = get_data('test-unlabelled.json','text')\n",
    "\n",
    "with open('articles.json') as f:\n",
    "    tr_scr_data = json.load(f)\n",
    "tr_label = [1]*len(tr_data) + [0]*len(tr_scr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_tr_data = tr_data + tr_scr_data\n",
    "bi_tr_label =tr_label\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "x_tr_tr, x_tr_dev, y_tr_tr, y_tr_dev = train_test_split(bi_tr_data, bi_tr_label, \n",
    "                                                        test_size=0.2, random_state=1015,\n",
    "                                                        stratify = bi_tr_label)\n",
    "tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(bi_tr_data)\n",
    "x_tr_all = tokenizer.texts_to_matrix(bi_tr_data, mode=\"tfidf\")\n",
    "x_tr_tr = tokenizer.texts_to_matrix(x_tr_tr, mode=\"tfidf\") \n",
    "x_tr_dev = tokenizer.texts_to_matrix(x_tr_dev, mode=\"tfidf\")\n",
    "x_dev = tokenizer.texts_to_matrix(dev_data, mode=\"tfidf\") \n",
    "x_comp = tokenizer.texts_to_matrix(test_data, mode=\"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 49674\n"
     ]
    }
   ],
   "source": [
    "vocab_size = x_tr_all.shape[1]\n",
    "print(\"Vocab size =\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  is: 0.8\n",
      "Macro F1  is: 0.7959183673469388\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x_tr_all, bi_tr_label)\n",
    "y_pred = classifier.predict(x_dev)\n",
    "acc= accuracy_score(dev_label , y_pred)\n",
    "f1= f1_score(dev_label ,y_pred)\n",
    "print('Accuracy  is: ' + str(acc))\n",
    "print('Macro F1  is: ' + str(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-Forwards NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"feedforward-bow-input\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 100)               4967500   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,987,801\n",
      "Trainable params: 4,987,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "#model definition\n",
    "model = Sequential(name=\"feedforward-bow-input\")\n",
    "model.add(layers.Dense(100, input_dim=vocab_size, activation='relu'))\n",
    "model.add(layers.Dense(100, input_dim=vocab_size, activation='relu'))\n",
    "model.add(layers.Dense(100, input_dim=vocab_size, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "#since it's a binary classification problem, we use a binary cross entropy loss here\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1824 samples, validate on 457 samples\n",
      "Epoch 1/10\n",
      "1824/1824 [==============================] - 8s 4ms/step - loss: 0.1788 - accuracy: 0.9386 - val_loss: 0.1561 - val_accuracy: 0.9694\n",
      "Epoch 2/10\n",
      "1824/1824 [==============================] - 6s 4ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.1747 - val_accuracy: 0.9650\n",
      "Epoch 3/10\n",
      "1824/1824 [==============================] - 5s 3ms/step - loss: 2.0423e-04 - accuracy: 1.0000 - val_loss: 0.1857 - val_accuracy: 0.9650\n",
      "Epoch 4/10\n",
      "1824/1824 [==============================] - 5s 3ms/step - loss: 9.4304e-05 - accuracy: 1.0000 - val_loss: 0.1961 - val_accuracy: 0.9650\n",
      "Epoch 5/10\n",
      "1824/1824 [==============================] - 6s 3ms/step - loss: 4.7974e-05 - accuracy: 1.0000 - val_loss: 0.2102 - val_accuracy: 0.9672\n",
      "Epoch 6/10\n",
      "1824/1824 [==============================] - 5s 3ms/step - loss: 2.6911e-05 - accuracy: 1.0000 - val_loss: 0.2199 - val_accuracy: 0.9672\n",
      "Epoch 7/10\n",
      "1824/1824 [==============================] - 5s 3ms/step - loss: 1.6278e-05 - accuracy: 1.0000 - val_loss: 0.2335 - val_accuracy: 0.9672\n",
      "Epoch 8/10\n",
      "1824/1824 [==============================] - 6s 3ms/step - loss: 1.0479e-05 - accuracy: 1.0000 - val_loss: 0.2421 - val_accuracy: 0.9650\n",
      "Epoch 9/10\n",
      "1824/1824 [==============================] - 6s 3ms/step - loss: 7.2197e-06 - accuracy: 1.0000 - val_loss: 0.2495 - val_accuracy: 0.9650\n",
      "Epoch 10/10\n",
      "1824/1824 [==============================] - 5s 3ms/step - loss: 5.1034e-06 - accuracy: 1.0000 - val_loss: 0.2554 - val_accuracy: 0.9650\n",
      "\n",
      "Testing models accuracy :  0.8200\n",
      "\n",
      "Testing acc:  0.8200\n",
      "\n",
      "Testing f1:  0.8163\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_tr_tr, y_tr_tr, epochs=10, verbose=True, validation_data=(x_tr_dev, y_tr_dev), \n",
    "          batch_size=10)\n",
    "\n",
    "loss, accuracy = model.evaluate(x_dev, dev_label, verbose=False)\n",
    "\n",
    "y_pred1 = model.predict(x_dev)\n",
    "y_pred = [int(round(elem[0])) for elem in y_pred1]\n",
    "\n",
    "acc1 = accuracy_score(dev_label, y_pred)\n",
    "f1_1 = f1_score(dev_label, y_pred)\n",
    "\n",
    "print(\"\\nTesting models accuracy :  {:.4f}\".format(accuracy))\n",
    "print(\"\\nTesting acc:  {:.4f}\".format(acc1))\n",
    "print(\"\\nTesting f1:  {:.4f}\".format(f1_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTSM NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_tr, x_tr_dev, y_tr_tr, y_tr_dev = train_test_split(bi_tr_data, bi_tr_label, \n",
    "                                                        test_size=0.2, random_state=1015,\n",
    "                                                        stratify = bi_tr_label)\n",
    "\n",
    "xseq_train = tokenizer.texts_to_sequences(x_tr_tr)\n",
    "xseq_dev = tokenizer.texts_to_sequences(x_tr_dev)\n",
    "xseq_test = tokenizer.texts_to_sequences(dev_data)\n",
    "xseq_comp = tokenizer.texts_to_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 818\n",
    "xseq_train = pad_sequences(xseq_train, padding='post',maxlen=maxlen )\n",
    "xseq_dev = pad_sequences(xseq_dev, padding='post',maxlen=maxlen )\n",
    "xseq_test = pad_sequences(xseq_test, padding='post',maxlen=maxlen)\n",
    "xseq_comp = pad_sequences(xseq_comp, padding='post',maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lstm\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 818, 500)          24837000  \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 818, 100)          240400    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 818, 100)          80400     \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 818, 100)          80400     \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 25,318,701\n",
      "Trainable params: 25,318,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "embedding_dim = 500\n",
    "\n",
    "#word order preserved with this architecture\n",
    "model3 = Sequential(name=\"lstm\")\n",
    "model3.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model3.add(LSTM(100, return_sequences=True))\n",
    "model3.add(LSTM(100, return_sequences=True))\n",
    "model3.add(LSTM(100, return_sequences=True))\n",
    "model3.add(LSTM(100))\n",
    "model3.add(layers.Dense(1, activation='sigmoid'))\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1824 samples, validate on 457 samples\n",
      "Epoch 1/10\n",
      "1824/1824 [==============================] - 362s 199ms/step - loss: 0.6913 - accuracy: 0.5411 - val_loss: 0.6797 - val_accuracy: 0.5711\n",
      "Epoch 2/10\n",
      "1824/1824 [==============================] - 201s 110ms/step - loss: 0.6512 - accuracy: 0.5855 - val_loss: 0.6294 - val_accuracy: 0.6280\n",
      "Epoch 3/10\n",
      "1824/1824 [==============================] - 181s 99ms/step - loss: 0.5853 - accuracy: 0.7582 - val_loss: 0.4301 - val_accuracy: 0.8249\n",
      "Epoch 4/10\n",
      "1824/1824 [==============================] - 156s 86ms/step - loss: 0.4025 - accuracy: 0.8618 - val_loss: 0.5255 - val_accuracy: 0.8446\n",
      "Epoch 5/10\n",
      "1824/1824 [==============================] - 159s 87ms/step - loss: 0.4464 - accuracy: 0.8558 - val_loss: 0.4859 - val_accuracy: 0.7812\n",
      "Epoch 6/10\n",
      "1824/1824 [==============================] - 158s 87ms/step - loss: 0.4305 - accuracy: 0.8218 - val_loss: 0.4655 - val_accuracy: 0.7965\n",
      "Epoch 7/10\n",
      "1824/1824 [==============================] - 164s 90ms/step - loss: 0.4892 - accuracy: 0.7615 - val_loss: 0.5218 - val_accuracy: 0.7309\n",
      "Epoch 8/10\n",
      "1824/1824 [==============================] - 159s 87ms/step - loss: 0.4443 - accuracy: 0.8026 - val_loss: 0.5078 - val_accuracy: 0.8096\n",
      "Epoch 9/10\n",
      "1824/1824 [==============================] - 156s 86ms/step - loss: 0.5292 - accuracy: 0.7889 - val_loss: 0.6098 - val_accuracy: 0.6652\n",
      "Epoch 10/10\n",
      "1824/1824 [==============================] - 157s 86ms/step - loss: 0.5567 - accuracy: 0.7029 - val_loss: 0.6181 - val_accuracy: 0.6368\n",
      "\n",
      "Testing acc:  0.6300\n",
      "\n",
      "Testing acc:  0.6300\n",
      "\n",
      "Testing f1:  0.7218\n"
     ]
    }
   ],
   "source": [
    "model3.fit(xseq_train, y_tr_tr, epochs=10, verbose=True, validation_data=(xseq_dev, y_tr_dev), batch_size=100)\n",
    "\n",
    "loss, accuracy = model3.evaluate(xseq_test, dev_label, verbose=False)\n",
    "\n",
    "y_pred1 = model3.predict(xseq_test)\n",
    "y_pred = [int(round(elem[0])) for elem in y_pred1]\n",
    "\n",
    "acc2 = accuracy_score(dev_label, y_pred)\n",
    "f1_2 = f1_score(dev_label, y_pred)\n",
    "\n",
    "print(\"\\nTesting acc:  {:.4f}\".format(acc2))\n",
    "print(\"\\nTesting f1:  {:.4f}\".format(f1_2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
